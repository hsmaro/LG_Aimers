# 온라인 채널 제품 판매량 예측 AI 오프라인 해커톤

> **LG Aimers 3기**

### 배경

- 온라인 판매 채널에서 수집되는 대규모 데이터를 통하여 온라인 판매 환경에서 인사이트를 도출하고 데이터를 활용하여 정확한 판매 예측을 수행

### 주제

- 온라인 채널 제품판 판매량 예측

### 설명

- 특정 온라인 쇼핑몰의 일별 제품 판매 데이터를 바탕으로 향후 21일의 제품별 판매량을 예측하는 AI 모델 개발

- 기간 : 23.09.16 ~ 23.09.17
- LG 인화원에서 1박 2일간 오프라인 해커톤을 통해 온라인 해커톤에서 문제 해결 능력이 검증된 33팀 중 최종 수상팀을 결정하기 위한 해커톤 (총 747팀 참여)
- **한 달간 진행하였던 온라인 해커톤을 통하여 오프라인 진출 (상위 7%)**

### Metric

- Pseudo SFA(PSFA) 
![image](https://github.com/Eastha0526/LG_aimers/assets/110336043/1e04b518-17e5-4299-ab7d-fcb3ae9e930f)

---
### Collaborator

- 팀 : 띵지

|이름|닉네임|프로필|
|:--:|:--:|:--:|
|권경민|km0228kr|https://github.com/km0228kr|
|김동하|Eastha0526|https://github.com/Eastha0526|
|차형석|hsmaro|https://github.com/hsmaro|

---

## 모델 구축

- 데이터의 경우 시퀀스 정보를 반영하기 위한 시계열 모델 활용
- 온라인 예선의 경우 LSTM과 LayerNorm을 활용한 모델을 통하여 오프라인에 진출
- 오프라인의 경우 각 팀당 서버 제공(p40 2대), 이를 통하여 더 깊은 아키텍처를 준비

#### 사용 모델

- Seq2Seq
    - Sequence To Sequence Learning With Neural Networks (https://arxiv.org/abs/1409.3215)
    - 기계 번역에 사용되는 모델, RNN에서 발생할 수 있는 LongTermDependency 문제를 해결하기 위하여 구성된 아키텍처
    - 입력 시퀀스를 통하여 컨텐스트 벡터를 생성하고 디코더의 경우 이 컨텍스트 벡터를 입력 받아 출력을 진행한다.
    - LSTM 레이어를 통하여 구성하였음
    - 베이스라인에서 예측에 활용한 윈도우 사이즈의 경우 90, 하지만 더 깊은 윈도우를 활용하여 예측을 한다면 더 정확한 예측이 가능할 것이라고 판단

- Seq2Seq with Attention
    - Neural Machine Translation by Jointly Learning to Align and Translate (https://arxiv.org/abs/1409.0473)
    - Seq2Seq 모델의 한계점 존재, 입력 시퀀스를 하나의 컨텍스트 벡터에 압축을 하다보니, 정보의 손실이 발생
    - 시퀀스의 길이가 길다면 정보의 손실이 더 커지게 된다.
    - 이러한 문제점을 해결하기 위하여 Seq2Seq with Attention 모델 사용
    - 어느 시점의 인코더의 은닉 상태에 '집중'을 해야하는지를 통해 디코더에서의 예측에 활용하기 때문에 보다 정확한 판매량 예측이 가능할 것이라고 판단
    - GRU와 LSTM 모델 모두 구성하였음
    - 또한 교사 강요를 통하여 모델의 학습속도를 높이려는 시도 또한 준비하였음
     
#### 메타 데이터 활용

- train 데이터와 함께 각 제품별 정보 데이터 존재
- 이 데이터의 경우 텍스트 마이닝을 활용하여 각 제품에서 필요한 정보 부분 추출
- 제품타입, 헤어타입, HCA, 700mg 등의 정보만 추출하여 조인을 통한 훈련 데이터와 결합
- 결측치 부분은 제거를 통하여 제품정보가 있는 데이터에 특정하여 예측
- 메타 데이터를 활용하기 전 모델의 경우 PSFA score의 경우 0.49, 0.5를 기록 -> 메타 데이터 활용 후 GRU 모델과 함께 사용했을 경우 0.55 까지 향상된 것을 확인

---

## 느낀점 & 배운점
- 또한 서버 제공을 해준다고 하여서 아키텍처도 고도화를 하였고, 모델들의 하이퍼파라미터 또한 큰 값을 디폴트 값으로 준비하였는데 실제 대회 현장에서 한 에폭당 시간이 6~8시간으로 발생하여서 모델의 크기 및 모델의 하이퍼파라미터를 최소한의 수준으로 튜닝을 해야하는 문제점도 발생하였다.
- 멘토님과의 대화를 통하여 서버에서는 하나의 큰 모델을 실행하고 개인 코랩을 활용하여 작은 모델들을 여러번 실험하면서 최적의 값을 찾으라는 조언을 받아 실제로 진행을 하였고, 최소한의 고정값인 옵티마이저와 학습률 스케쥴러의 경우 모든 팀원이 같이 고정하고 그 이외의 하이퍼파라미터들을 변경하면서 최종 0.55의 양방향 GRU 모델을 통하여 예측에 성공
- 1박 2일간 진행되는 시간이 제한적인 해커톤에서 팀원들간의 의견 교류를 해가면서 서로 로깅을 철저하게 하면서 진행하는 것의 중요성을 배울 수 있었으며, 서로 밤새 (전날 13시 ~ 다음날 10시) 약 20시간 동안 쉬지않고 모델 학습을 하였고 최종적으로 만족할만한 모델 결과를 제작하였다.

---
- 최종 수상팀 (총 3팀)들의 발표에서를 통하여 문제점을 파악할 수 있었다.
- 최종 수상팀들의 경우 모델 아키텍처를 고도화 하는 것 보다 모두 데이터 전처리 및 EDA에 더 신경을 썼다고 발표하였다. 실제로 1위팀의 경우 베이스라인 모델 아키텍처를 그대로 사용하였음에도 불구하고 PSFA 스코어 값을 0.6을 달성하였다.
- Imputation 및 추가 파생변수 제작에 더 힘을 쏟는 상위권 팀들을 확인할 수 있었다.
- 도메인 지식을 활용하여 상위권에 달성한 팀도 존재하였는데, 잘팔리는 제품 20%가 시장 매출의 80%를 차지한다는 파레토 법칙을 활용하여 잘팔리는 판매량에 대하여 가중치를 부여하여 더 예측 정확도를 올린 팀의 분석 내용 또한 배울점이 많았다.
- 우리 팀의 경우, 모델 아키텍처 및 하이퍼파라미터 최적화를 통해 모델적인 부분만을 강화하는 시도를 하였는데 이런 시도가 아닌 전처리 및 EDA를 통해 데이터를 더 파악하며 데이터 부분 또한 더 신경을 써서 모델링에 활용하는 것이 중요하다는 사실을 배울 수 있었다.
- 또한, 결국 준비한 모델이 너무 무거워서 제한된 20시간이라는 시간안에 실험을 여러번 할 수 없어서 아쉬움이 남았다. 오컴의 면도날 이론에서 알 수 있는 것처럼, 너무 복잡하게만 생각하는 것이 아닌 단순하게 생각하는 것이 정답에 가까울 수 도 있다라는 것 또한 배울 수 있던 해커톤 이였던 것 같다.
- 실제로 우리의 최종 제출횟수는 22회 뿐이였고, 1위팀은 200번, 그 외의 상위 팀 또한 거의 100회 이상 제출을 하였음을 통해 좀 더 상황에 맞는 모델링을 진행하여야 한다는 것도 배울 수 있었다.
- 앞으로, 데이터 EDA 및 파생변수 생성과 도메인 지식을 활용한 데이터 이해 부분에 더욱 신경을 쓰면서 모델도 적절한 태스크에 맞게 제적을 해야한다고 생각한다.
