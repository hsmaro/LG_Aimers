# 온라인 채널 제품 판매량 예측 AI 오프라인 해커톤

> **LG Aimers 3기**

### 배경

- 온라인 판매 채널에서 수집되는 대규모 데이터를 통하여 온라인 판매 환경에서 인사이트를 도출하고 데이터를 활용하여 정확한 판매 예측을 수행

### 주제

- 온라인 채널 제품판 판매량 예측

### 설명

- 특정 온라인 쇼핑몰의 일별 제품 판매 데이터를 바탕으로 향후 21일의 제품별 판매량을 예측하는 AI 모델 개발

- 기간 : 23.09.16 ~ 23.09.17
- LG 인화원에서 1박 2일간 오프라인 해커톤을 통해 온라인 해커톤에서 문제 해결 능력이 검증된 33팀 중 최종 수상팀을 결정하기 위한 해커톤 (총 747팀 참여)
- **한 달간 진행하였던 온라인 해커톤을 통하여 오프라인 진출 (상위 7%)**

### Metric

- Pseudo SFA(PSFA) 
- ![image](https://github.com/Eastha0526/LG_aimers/assets/110336043/8b94481f-bda6-4c44-9f7e-04f6d3c6b625)


---
### Collaborator

- 팀 : 띵지

|이름|닉네임|프로필|
|:--:|:--:|:--:|
|권경민|km0228kr|https://github.com/km0228kr|
|김동하|Eastha0526|https://github.com/Eastha0526|
|차형석|hsmaro|https://github.com/hsmaro|

---

## 해결 방법

- 데이터의 경우 시퀀스 정보를 반영하기 위한 시계열 모델 활용
- 온라인 예선의 경우 LSTM과 LayerNorm을 활용한 모델을 통하여 오프라인에 진출
- 오프라인의 경우 각 팀당 서버 제공(p40 2대), 이를 통하여 더 깊은 아키텍처를 준비

#### 사용 모델

- Seq2Seq
    - Sequence To Sequence Learning With Neural Networks (https://arxiv.org/abs/1409.3215)
    - 기계 번역에 사용되는 모델, RNN에서 발생할 수 있는 LongTermDependency 문제를 해결하기 위하여 구성된 아키텍처
    - 입력 시퀀스를 통하여 컨텐스트 벡터를 생성하고 디코더의 경우 이 컨텍스트 벡터를 입력 받아 출력을 진행한다.
    - LSTM 레이어를 통하여 구성하였음
    - 베이스라인에서 예측에 활용한 윈도우 사이즈의 경우 90, 하지만 더 깊은 윈도우를 활용하여 예측을 한다면 더 정확한 예측이 가능할 것이라고 판단

- Seq2Seq with Attention
    - Neural Machine Translation by Jointly Learning to Align and Translate (https://arxiv.org/abs/1409.0473)
    - Seq2Seq 모델의 한계점 존재, 입력 시퀀스를 하나의 컨텍스트 벡터에 압축을 하다보니, 정보의 손실이 발생
    - 시퀀스의 길이가 길다면 정보의 손실이 더 커지게 된다.
    - 이러한 문제점을 해결하기 위하여 Seq2Seq with Attention 모델 사용
    - 어느 시점의 인코더의 은닉 상태에 '집중'을 해야하는지를 통해 디코더에서의 예측에 활용하기 때문에 보다 정확한 판매량 예측이 가능할 것이라고 판단
    - GRU와 LSTM 모델 모두 구성하였음
    - 또한 교사 강요를 통하여 모델의 학습속도를 높이려는 시도 또한 준비하였음

- Transformer
    - Attention is all you need (https://arxiv.org/abs/1706.03762)
    - RNN 모델의 경우 병렬화가 불가능하다는 문제 및 LongTermDependency 문제가 존재하였다.
    - 이러한 LongTermDependency의 경우 어텐션 메커니즘을 통하여 해결할 수 있었고, 밀집층과 어텐션 연산만으로도 예측에 좋은 결과를 낼 수 있는 트랜스 포머 모델을 활용하였음
    - 이번 데이터의 경우 판매량 예측이고, 시퀀스 정보를 입력해주어야 하기 때문에 포지셔널 인코딩활용
        - 포지셔널 인코딩의 경우 논문에서 제시된 사인과 코사인을 활용

- LSTF-Linear
    - Are Transformers Effective for Time Series Forecasting? (https://arxiv.org/abs/2205.13504)
    - SOTA (State of the art) 모델인 LSTF-Linear 모델을 활용
    - 논문의 저자들의 경우 실제 트랜스포머 기반의 모델이 장기 시계열 예측에 효과적이지 않고 오히려 단순한 모델이 더 성능이 좋다고 주장
    - Linear, NLinear, DLinear 총 3가지 모델을 준비
    - ![image](https://github.com/Eastha0526/LG_aimers/assets/110336043/e7fb53d3-e2a7-4295-a39c-dfd86c246118)
    - Long Term Time Series Forecasting (LTSF) 과제에서 시간적 변화를 모델링 하는데 주 목적을 두기 때문에 시퀀스 정보가 예측에 있어 가장 중요하다.
    - 이러한 시퀀스 정보를 위하여 트랜스포머에서는 포지셔널 인코딩 기법을 사용하였다.
        - 하지만 포지셔널 인코딩 이후 진행되는 멀티 헤드 셀프 어텐션은 시퀀스에 대한 정보 손실이 있을 수 밖에 없다. 트랜스포머가 주로 사용되는 NLP 분야에서는 문장내 시퀀스가 의미자체에 영향을 주지 않기 때문에 약간의 손실이 크게 작용하지 않는다.
          - 예 : 나는 소년 이다. 나는 이다 소년
        - 하지만 추세 및 주기성이 존재하는 시계열 데이터의 경우 이런 정보손실은 큰 문제를 발생 할 수 있다.
        - -> 이러한 관점에서 트랜스포머 모델이 실제로 성능이 과장되었다고 생각하고 진행하였다고 함

    - DLinear
      - ![image](https://github.com/Eastha0526/LG_aimers/assets/110336043/d6ffd489-70e6-4979-a544-7c3e4af5b55d)
      - Autoformer 및 FEDformer에서 사용되는 시계열 분해 방식을 사용한 모델
      - 이동 평균값을 구하고 추세와 주기성 데이터로 분해하여 각각 선형 레이어를 통해 적용하여 결합 후 마지막 예측을 계산
        
    - NLinear

      - 상승하거나 하락하는 추세를 지녔을 경우 학습 데이터의 평균과 분산으로 데이터를 정규화한다면 평가 데이터에 분포 이동이 발생할 수 있다.
      - 이럴 경우 학습된 모형의 예측 값은 분포에서 크게 벗어나기 때문에 예측 성능이 하락한다.
      - 따라서 이를 개선하기 위하여 가장 마지막 값을 빼서 모델을 학습시키고 가장 마지막에 다시 그 값을 더해서 실제 값이 존재하는 분포로 이동시킨다
      - 이러한 방법을 적용하면서 현재 시계열 예측 모델 1위를 유지중
     
#### 메타 데이터 활용

- train 데이터와 함께 각 제품별 정보 데이터 존재
- 이 데이터의 경우 텍스트 마이닝을 활용하여 각 제품에서 필요한 정보 부분 추출
- 제품타입, 헤어타입, HCA, 700mg 등의 정보만 추출하여 조인을 통한 훈련 데이터와 결합
- 결측치 부분은 제거를 통하여 제품정보가 있는 데이터에 특정하여 예측
- 메타 데이터를 활용하기 전 모델의 경우 PSFA score의 경우 0.49, 0.5를 기록 -> 메타 데이터 활용 후 GRU 모델과 함께 사용했을 경우 0.55 까지 향상된 것을 확인

---

## 배운점

- 모델을 준비하였을 때, 분산 컴퓨팅 방식 중 DDP(Distributed Data Parralle)를 사용하여 준비를 하였음, 실제 로컬에서 실험을 해볼 수 없어서 테스트를 해보지 못하였는데, 실제 대회를 진행할 때 이 부분에서 에러가 발생하여 시간을 너무 많이 소비하였고 결국 단순 DP(Data Parralle)로 밖에 구성하지 못하였다.
- 또한 서버 제공을 해준다고 하여서 아키텍처도 고도화를 하였고, 모델들의 하이퍼파라미터 또한 큰 값을 디폴트 값으로 준비하였는데 실제 대회 현장에서 한 에폭당 시간이 6~8시간으로 발생하여서 모델의 크기 및 모델의 하이퍼파라미터를 최소한의 수준으로 튜닝을 해야하는 문제점도 발생하였다.
- 멘토님과의 대화를 통하여 서버에서는 하나의 큰 모델을 실행하고 개인 코랩을 활용하여 작은 모델들을 여러번 실험하면서 최적의 값을 찾으라는 조언을 받아 실제로 진행을 하였고, 최소한의 고정값인 옵티마이저와 학습률 스케쥴러의 경우 모든 팀원이 같이 고정하고 그 이외의 하이퍼파라미터들을 변경하면서 최종 0.55의 양방향 GRU 모델을 통하여 예측에 성공
- 1박 2일간 진행되는 시간이 제한적인 해커톤에서 팀원들간의 의견 교류를 해가면서 서로 로깅을 철저하게 하면서 진행하는 것의 중요성을 배울 수 있었으며, 서로 밤새 (전날 13시 ~ 다음날 10시) 약 20시간 동안 쉬지않고 모델 학습을 하였고 최종적으로 만족할만한 모델 결과를 제작하였다.

---
- 최종 수상팀 (총 3팀)들의 발표에서를 통하여 문제점을 파악할 수 있었다.
- 최종 수상팀들의 경우 모델 아키텍처를 고도화 하는 것 보다 모두 데이터 전처리 및 EDA에 더 신경을 썼다고 발표하였다. 실제로 1위팀의 경우 베이스라인 모델 아키텍처를 그대로 사용하였음에도 불구하고 PSFA 스코어 값을 0.6을 달성하였다.
- Imputation 및 추가 파생변수 제작에 더 힘을 쏟는 상위권 팀들을 확인할 수 있었다.
- 도메인 지식을 활용하여 상위권에 달성한 팀도 존재하였는데, 잘팔리는 제품 20%가 시장 매출의 80%를 차지한다는 파레토 법칙을 활용하여 잘팔리는 판매량에 대하여 가중치를 부여하여 더 예측 정확도를 올린 팀의 분석 내용 또한 배울점이 많았다.
- 우리 팀의 경우, 모델 아키텍처 및 하이퍼파라미터 최적화를 통해 모델적인 부분만을 강화하는 시도를 하였는데 이런 시도가 아닌 전처리 및 EDA를 통해 데이터를 더 파악하며 데이터 부분 또한 더 신경을 써서 모델링에 활용하는 것이 중요하다는 사실을 배울 수 있었다.
- 또한, 결국 준비한 모델이 너무 무거워서 제한된 20시간이라는 시간안에 실험을 여러번 할 수 없어서 아쉬움이 남았다. 오컴의 면도날 이론에서 알 수 있는 것처럼, 너무 복잡하게만 생각하는 것이 아닌 단순하게 생각하는 것이 정답에 가까울 수 도 있다라는 것 또한 배울 수 있던 해커톤 이였던 것 같다.
- 실제로 우리의 최종 제출횟수는 22회 뿐이였고, 1위팀은 200번, 그 외의 상위 팀 또한 거의 100회 이상 제출을 하였음을 통해 좀 더 상황에 맞는 모델링을 진행하여야 한다는 것 도 배울 수 있었다.
- 앞으로, 데이터 EDA 및 파생변수 생성과 도메인 지식을 활용한 데이터 이해 부분에 더욱 신경을 쓰면서 모델도 적절한 태스크에 맞게 제적을 해야한다고 생각한다.
